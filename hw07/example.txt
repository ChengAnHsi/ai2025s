========== The 1-st segment of the split (505 words) ==========


 Decision tree organizes a series of rules in the tree structure.  It is one of

the most practical methods for non-parametric supervised learning.  Our goal in

this video is to demonstrate how to create a decision tree that predicts the

value of a target  by learning decision rules inferred from the training data.

Let's first have a look at some important terminology of decision tree.  The

root node is at the beginning of a tree.  It represents the entire population

being analyzed.  Splitting is the


========== The 2-nd segment of the split (507 words) ==========


process of dividing a node into subnodes.  When a subnode can be splitted into

further nodes, then it is called decision nodes.  Leaf nodes are the nodes of a

tree that have no further splitted nodes.  When we remove subnodes of a decision

node, this process is called pruning.  This is the opposite process of

splitting, which is used to prevent overfitting.  Now let's demonstrate the

process of creating a decision tree from data.  First of all, let's inspect the

dataset.  In this example, there are two


========== The 3-rd segment of the split (506 words) ==========


classes and two features, size and color, for the training data.  We can

construct some decision rules from the features.  For example, here are two

simple decision rules.  The first is whether the set is greater or equal to 2,

and the second is whether the color is yellow or not.  In real application,

there are usually a lot of decision rules we can choose from,  so we need to

find a way to choose the best decision rule to split the current node.  Starting

from the root node, we are going to iterate


========== The 4-th segment of the split (507 words) ==========


through all the decision rules  and calculate the information gain of the

current split.  Calculating information gain is the centerpiece of this step,

so what is information gain?  Let's have a closer look at each group within a

decision rule.  The group is pure when all of its records belong to the same

class.  Here we use gaining impurity to measure how pure the current node is.

When the group is pure, gaining impurity is 0.  When the group is half and half

mixed, gaining impurity is 0.5.  For any


========== The 5-th segment of the split (511 words) ==========


decision split, a parent group is split into two child groups.  The information

gain of this split is the impurity of the parent group  minus weighted average

impurity of the child group.  Once we calculate the information gain of each

splitting decision rule,  we now can select the decision rule which has the

largest information gain  to split the current node.  Then we can split the

child node recursively,  and only considering decision rule never selected

before in the current branch.  We stop splitting


========== The 6-th segment of the split (511 words) ==========


if there are no decision rules left or the group impurity is 0.  Sometimes

pruning and early stop conditions are used  to prevent a large number of splits

and overfitting.  Now the decision tree can predict the probability of each

class  as the ratio of each class in the node.  In this case, for the record, we

set greater or equal to 2  and the color is yellow, there is a 50% chance the

class record is a circle.  So that is a brief demonstration of how to construct

decision tree from data.  It has a lot of


========== The 7-th segment of the split (508 words) ==========


advantages like easy to interpret  and straightforward for visualizations.

However, it has a big disadvantage.  Single decision tree model are prone to

overfitting,  especially when a tree is particularly steep.  One way to combat

this issue is by setting a max depth of the tree.  This will limit the risk of

overfitting,  but this will be at the expense of increasing bias.  Random forest

is a good way to prevent overfitting with auto-sacrificing bias.  It is simply a

collection of decision trees  whose


========== The 8-th segment of the split (347 words) ==========


results are aggregated into one final result.  It is a strong ensemble modeling

technique  and much more robust than a single decision tree.  So in real-world

application,  people usually go directly into using random forest for modeling.

I hope you like this video.  Don't forget to subscribe to us and ring the

notification bell and stay tuned.